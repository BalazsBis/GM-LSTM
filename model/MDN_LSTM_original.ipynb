{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Gumbel\n",
    "from typing import List, Dict\n",
    "from itertools import groupby\n",
    "\n",
    "#Import necessary functions\n",
    "sys.path.append(\"aux_functions\")\n",
    "from functions_datasets_attert import attert_model as DataBase # define what you import as DataBase!\n",
    "from functions_datasets_attert import validate_samples, manual_train_test_split\n",
    "from functions_loss_attert import gaussian_distribution, nll_loss\n",
    "from functions_aux_attert import create_folder, set_random_seed, write_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. Initialize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to access the information\n",
    "path_entities = #path to import the different locations IDs; .txt \n",
    "path_data = # data folder path\n",
    "\n",
    "# dynamic forcings and target\n",
    "dynamic_input = ['P', 'ET', 'T', 'Q', 'Sensor_depth']#, 'Sensor_type_A', 'Sensor_type_B', 'Sensor_type_C', 'Sensor_type_X'] # Sensor ID information (i.e. senyor type) is used for single-Gaussian LSTM, but not for three-Gaussian LSTM\n",
    "\n",
    "target = ['VWC']\n",
    "\n",
    "# static attributes that will be used\n",
    "static_input = ['dem',\n",
    "       'landuse_A', 'landuse_B', 'landuse_C', 'landuse_D', 'landuse_E',\n",
    "       'landuse_F', 'soiltype_A', 'soiltype_B', 'soiltype_C', 'soiltype_D']\n",
    "\n",
    "# time periods\n",
    "time_period = 2160 # Time period for segmentationg lenght\n",
    "random_state_separation = 5 # Random seed to ensure different training and testing sets for each initialization\n",
    "\n",
    "\n",
    "model_hyper_parameters = {\n",
    "    \"input_size\": len(dynamic_input) + len(static_input),\n",
    "    \"no_of_layers\":1,  \n",
    "    \"seq_length\": 128,\n",
    "    \"hidden_size\": 64,\n",
    "    \"batch_size\": 128,\n",
    "    \"no_of_epochs\": 10,             \n",
    "    \"drop_out\": 0.4, \n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"adapt_learning_rate_epoch\": 1,\n",
    "    \"adapt_gamma_learning_rate\": 0.5,\n",
    "    \"set_forget_gate\":3\n",
    "}\n",
    "\n",
    "# device to train the model\n",
    "running_device = 'cpu' #cpu or gpu\n",
    "# define random seed\n",
    "seed = 42\n",
    "# Name of the folder where the results will be stored \n",
    "path_save_folder = #''\n",
    "\n",
    "# colorblind friendly palette for plotting\n",
    "color_palette = {'observed': '#1f78b4','LSTM': '#ff7f00'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'attert_model/saved_epochs/temporal_segmentation/run_12' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create folder to store the results\n",
    "create_folder(folder_path=path_save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Class to create the dataset object used to manage the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "\n",
    "    #Function to initialize the data\n",
    "    def __init__(self, \n",
    "                 dynamic_input: List[str],\n",
    "                 static_input: List[str],\n",
    "                 set_type: str,\n",
    "                 target: List[str], \n",
    "                 sequence_length: int,\n",
    "                 path_entities: str,\n",
    "                 path_data: str,\n",
    "                 forcings: List[str] = [],\n",
    "                 check_NaN:bool = True\n",
    "                 ):\n",
    "\n",
    "        # read and create variables\n",
    "        self.time_period = time_period # time period that is being considere        \n",
    "        self.dynamic_input = dynamic_input  # dynamic forcings going as inputs of in the lstm\n",
    "        self.target = target  # target variable\n",
    "        self.sequence_length = sequence_length # sequence length\n",
    "        entities_ids = np.loadtxt(path_entities, dtype='str').tolist() \n",
    "        \n",
    "        # save the cathments as a list even if there is just one\n",
    "        self.entities_ids = [entities_ids] if isinstance(entities_ids, str) else entities_ids # catchments\n",
    "        \n",
    "        self.sequence_data = {} # store information that will be used to run the lstm\n",
    "        self.df_ts = {} # store processed dataframes for all basins\n",
    "        self.scaler = {} # information to standardize the data \n",
    "        self.location_std = {} # std of the target variable of each basin (can be used later in the loss function)\n",
    "        self.valid_entities= [] # list of the elements that meet the criteria to be used by the lstm\n",
    "        self.set_type = set_type\n",
    "        # Process the attributes\n",
    "        self.static_input = static_input # static attributes going as inputs to the lstm\n",
    "        if static_input:\n",
    "            self.df_attributes = self._load_attributes(path_data)\n",
    "\n",
    "\n",
    "        for id_sensordepth in self.entities_ids:\n",
    "            # Read files\n",
    "\n",
    "            df_ts = self._load_data(path_data=path_data, location_id=id_sensordepth, forcings=forcings)\n",
    "             # Calculate the number of x (selected) - hour elements\n",
    "            num_segments = len(df_ts) // time_period\n",
    "             # Create segments of selected hours\n",
    "            segments = [df_ts.iloc[i * time_period:(i + 1) * time_period].copy() for i in range(num_segments)]\n",
    "\n",
    "            # Add ID_sep column to each segment (it will be beneficial to be able to make a difference between segments)\n",
    "            for i, segment in enumerate(segments):\n",
    "                    segment['ID_sep'] = f\"{segment['ID_sensordepth'].iloc[0]}_{i+1}\"\n",
    "\n",
    "            # Example usage:\n",
    "            all_segments, fixed_test_segments = manual_train_test_split(segments, test_size=0.4, random_state=random_state_separation)\n",
    "\n",
    "            # Now process the data based on set_type\n",
    "            if set_type == \"train\":\n",
    "                segments = all_segments\n",
    "            elif set_type == \"test\":\n",
    "                segments = fixed_test_segments\n",
    "            else:\n",
    "                raise ValueError(\"Invalid set_type. Allowed values are 'train', 'validation', or 'test'.\")\n",
    "\n",
    "            for seg in segments:\n",
    "                id_sep = seg['ID_sep'].iloc[0]\n",
    "                id = seg['ID_sensordepth'].iloc[0]\n",
    "                df_ts = seg\n",
    "\n",
    "                # Checks for invalid samples due to NaN or insufficient sequence length\n",
    "                flag = validate_samples(x=df_ts.loc[:, self.dynamic_input].values, \n",
    "                                            y=df_ts.loc[:, self.target].values, \n",
    "                                            attributes=self.df_attributes.loc[id].values if self.static_input else None, \n",
    "                                            seq_length=self.sequence_length,\n",
    "                                             check_NaN=check_NaN)\n",
    "            \n",
    "                # Create a list that contains the indexes (basin, day) of the valid samples\n",
    "                valid_samples = np.argwhere(flag == 1)\n",
    "\n",
    "                self.valid_entities.extend([(id_sep, int(f[0])) for f in valid_samples])\n",
    "            \n",
    "                # Only store data if this basin has at least one valid sample in the given period\n",
    "                if valid_samples.size > 0:\n",
    "                    self.df_ts[id_sep] = df_ts\n",
    "                \n",
    "                    # Create dictionary entry for the basin\n",
    "                    self.sequence_data[id_sep] = {}\n",
    "\n",
    "                    # Store the information of the basin in a nested dictionary\n",
    "                    self.sequence_data[id_sep]['x_d'] = torch.tensor(df_ts.loc[:, self.dynamic_input].values, dtype=torch.float32)\n",
    "                    self.sequence_data[id_sep]['y'] = torch.tensor(df_ts.loc[:, self.target].values, dtype=torch.float32)\n",
    "                    if self.static_input:\n",
    "                        self.sequence_data[id_sep]['x_s'] = torch.tensor(self.df_attributes.loc[id].values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "               \n",
    "    def __len__(self):\n",
    "        return len(self.valid_entities)\n",
    "    \n",
    "    def __getitem__(self, id_sensor):\n",
    "        \"\"\"This function is used by PyTorch's dataloader to extract the information\"\"\"\n",
    "        location, i = self.valid_entities[id_sensor]\n",
    "\n",
    "        # tensor of inputs\n",
    "        x_LSTM = self.sequence_data[location]['x_d'][i-self.sequence_length+1:i+1, :]\n",
    "        if self.static_input:\n",
    "            x_s = self.sequence_data[location]['x_s'].repeat(x_LSTM.shape[0],1)\n",
    "            x_LSTM = torch.cat([x_LSTM, x_s], dim=1)\n",
    "        \n",
    "        # tensor of outputs\n",
    "        y_obs = self.sequence_data[location]['y'][i]\n",
    "\n",
    "        # optional also return the basin_std\n",
    "        if self.location_std:\n",
    "            return x_LSTM, y_obs, self.location_std[location].unsqueeze(0)\n",
    "        else:\n",
    "            return x_LSTM, y_obs\n",
    "\n",
    "    def _load_attributes(self, path_data: str) -> pd.DataFrame:\n",
    "        \"\"\"Call the specific function that reads the static attributes information.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_data : str\n",
    "            path to the folder were the data is stored\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        df_attributes: pd.DataFrame\n",
    "            Dataframe containing the attributes of interest for the catchments of interest\n",
    "        \"\"\"\n",
    "        df_attributes = DataBase.read_attributes(path_data=path_data)\n",
    "        df_attributes = df_attributes.loc[self.entities_ids, self.static_input]\n",
    "        return df_attributes\n",
    "\n",
    "\n",
    "    def _load_data(self, path_data: str, location_id:str, forcings:List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Call the specific function that reads a specific location timeseries into a dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_data : str\n",
    "            path to the folder were the data is stored.\n",
    "        location_id : str\n",
    "            location_id.\n",
    "        forcings : str\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df: pd.DataFrame\n",
    "            Dataframe with the locations` timeseries\n",
    "        \"\"\"\n",
    "        df_ts = DataBase.read_data(path_data=path_data, location_id=location_id, forcings = forcings)\n",
    "        return df_ts\n",
    "\n",
    "\n",
    "    def calculate_location_std(self):\n",
    "        \"\"\"Fill the self.location_std dictionary with the standard deviation of the target variables for each location\"\"\"\n",
    "        for id, data in self.sequence_data.items():\n",
    "            self.location_std[id] = torch.tensor(np.nanstd(data['y'].numpy()), dtype=torch.float32)\n",
    "    \n",
    "    def calculate_global_statistics(self):\n",
    "        \"\"\"Fill the self.scalar dictionary \n",
    "        \n",
    "        The function calculates the global mean and standard deviation of the dynamic inputs, target variables and \n",
    "        static attributes, and store the in a dictionary. It will be used later to standardize used in the LSTM. This\n",
    "        function should be called only in training period. \n",
    "        \"\"\"\n",
    "        global_x = np.vstack([df.loc[:, self.dynamic_input].values for df in self.df_ts.values()])\n",
    "        self.scaler['x_d_mean'] = torch.tensor(np.nanmean(global_x, axis=0), dtype=torch.float32)\n",
    "        self.scaler['x_d_std'] = torch.tensor(np.nanstd(global_x, axis=0), dtype=torch.float32)\n",
    "        del global_x\n",
    "\n",
    "        global_y = np.vstack([df.loc[:, self.target].values for df in self.df_ts.values()])\n",
    "        self.scaler['y_mean'] = torch.tensor(np.nanmean(global_y, axis=0), dtype=torch.float32)\n",
    "        self.scaler['y_std'] = torch.tensor(np.nanstd(global_y, axis=0), dtype=torch.float32)\n",
    "        del global_y\n",
    "\n",
    "        if self.static_input:\n",
    "            self.scaler['x_s_mean'] = torch.tensor(self.df_attributes.mean().values, dtype= torch.float32)\n",
    "            self.scaler['x_s_std'] = torch.tensor(self.df_attributes.std().values, dtype= torch.float32)\n",
    "    \n",
    "    def standardize_data(self, standardize_output:bool=True):\n",
    "        \"\"\"Standardize the data used in the LSTM. \n",
    "\n",
    "        The function standardize the data contained in the self.sequence_data dictionary \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        standardize_output : bool\n",
    "            Boolean to define if the output should be standardize or not. \n",
    "        \"\"\"\n",
    "        for location in self.sequence_data.values():\n",
    "            # Standardize input\n",
    "            location['x_d'] = (location['x_d'] - self.scaler['x_d_mean']) / (self.scaler['x_d_std'])\n",
    "            if self.static_input:\n",
    "                location['x_s'] = (location['x_s'] - self.scaler['x_s_mean']) / (self.scaler['x_s_std'])\n",
    "            # Standardize output\n",
    "            if standardize_output:\n",
    "                location['y'] = (location['y'] - self.scaler['y_mean']) / (self.scaler['y_std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Create the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset training\n",
    "training_dataset = BaseDataset(dynamic_input=dynamic_input,\n",
    "                               static_input=static_input,\n",
    "                               target=target,\n",
    "                               sequence_length=model_hyper_parameters['seq_length'],\n",
    "                               path_entities=path_entities,\n",
    "                               set_type='train',\n",
    "                               path_data=path_data,\n",
    "                               check_NaN=True)\n",
    "\n",
    "training_dataset.calculate_global_statistics() # the global statistics are calculated in the training period!\n",
    "training_dataset.standardize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_locations_train = [next(group)[0] for key, group in groupby(training_dataset.valid_entities, key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset validation\n",
    "validation_dataset = BaseDataset(dynamic_input=dynamic_input,\n",
    "                                 static_input=static_input,\n",
    "                                 target=target,\n",
    "                                 sequence_length=model_hyper_parameters['seq_length'],\n",
    "                                 path_entities=path_entities,\n",
    "                                 set_type='train',\n",
    "                                 path_data=path_data,\n",
    "                                 check_NaN=False)\n",
    "\n",
    "validation_dataset.scaler = training_dataset.scaler # read the global statisctics calculated in the training period\n",
    "validation_dataset.standardize_data(standardize_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4. Create the different dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches in training:  21680\n",
      "x_lstm: torch.Size([128, 128, 16]) | y: torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader for training data.\n",
    "train_loader = DataLoader(training_dataset, \n",
    "                          batch_size=model_hyper_parameters['batch_size'],\n",
    "                          shuffle=True,\n",
    "                          drop_last = True)\n",
    "\n",
    "print('Batches in training: ', len(train_loader))\n",
    "x_lstm, y = next(iter(train_loader))\n",
    "print(f'x_lstm: {x_lstm.shape} | y: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches in validation:  1365\n",
      "x_lstm: torch.Size([2033, 128, 16]) | y: torch.Size([2033, 1])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader for validation data.\n",
    "validation_batches=[[index for index, _ in group] for _ , group in groupby(enumerate(validation_dataset.valid_entities), \n",
    "                                                                           lambda x: x[1][0])] # each basin is one batch\n",
    "\n",
    "validation_loader = DataLoader(dataset=validation_dataset,\n",
    "                               batch_sampler=validation_batches)\n",
    "\n",
    "# see if the batches are loaded correctly\n",
    "print('Batches in validation: ', len(validation_loader))\n",
    "x_lstm, y= next(iter(validation_loader))\n",
    "print(f'x_lstm: {x_lstm.shape} | y: {y.shape}')\n",
    "\n",
    "# create some lists with the valid basins and the valid entities per basin that will help later to organize the data\n",
    "valid_locations = [next(group)[0] for key, group in groupby(validation_dataset.valid_entities, key=lambda x: x[0])]\n",
    "valid_entity_per_location = [[id for _, id in group] for key, group in groupby(validation_dataset.valid_entities, \n",
    "                                                                            key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5. Define GM-LSTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if model will be run in gpu or cpu\n",
    "if running_device == 'gpu':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device= f'cuda:0'\n",
    "elif running_device == 'cpu':\n",
    "    device = \"cpu\"\n",
    "\n",
    "class Cuda_MDN_LSTM(nn.Module):\n",
    "    def __init__(self, model_hyper_parameters, num_mixtures=3):\n",
    "        super().__init__()\n",
    "        self.num_features = model_hyper_parameters['input_size']\n",
    "        self.hidden_units = model_hyper_parameters['hidden_size']\n",
    "        self.num_layers = model_hyper_parameters['no_of_layers']\n",
    "        self.num_mixtures = num_mixtures\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=model_hyper_parameters['input_size'], \n",
    "                            hidden_size=model_hyper_parameters['hidden_size'], \n",
    "                            batch_first=True,\n",
    "                            num_layers=model_hyper_parameters['no_of_layers'])\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(model_hyper_parameters['drop_out'])\n",
    "\n",
    "        \n",
    "        self.linear_pi = nn.Linear(in_features=model_hyper_parameters['hidden_size'], out_features=num_mixtures)\n",
    "        self.linear_mu = nn.Linear(in_features=model_hyper_parameters['hidden_size'], out_features=num_mixtures)\n",
    "        self.linear_sigma = nn.Linear(in_features=model_hyper_parameters['hidden_size'], out_features=num_mixtures)\n",
    "           \n",
    "    def forward(self, x):\n",
    "        # initialize hidden state with zeros\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units, \n",
    "                         requires_grad=True, dtype=torch.float32, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units, \n",
    "                         requires_grad=True, dtype=torch.float32, device=x.device)\n",
    "        \n",
    "        out, (hn_1, cn_1) = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]  # sequence to one\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        pi = F.softmax(self.linear_pi(out), dim=-1)  # Gumbel softmax\n",
    "        \n",
    "        mu = self.linear_mu(out) # Calculating the means of the Gaussian distributions. The linear transformation maps the output to the space of mean values for each Gaussian component\n",
    "        sigma = torch.exp(self.linear_sigma(out))  # Computing standard deviations of the Gaussian distributions. Using exponential to ensure positivity - requirement for valid Gaussian distributions\n",
    "        \n",
    "        return pi, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization factor\n",
    "'''To ensure that the total probability under the distribution integrates to 1'''\n",
    "oneDivSqrtTwoPI = 1.0 / np.sqrt(2.0*np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6. Construct and run GM-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | Loss training: 0.682 | LR: 0.00050 | Training time: 1190.3 s\n",
      "Epoch: 2  | Loss training: 0.497 | LR: 0.00025 | Training time: 1133.4 s\n",
      "Epoch: 3  | Loss training: 0.441 | LR: 0.00013 | Training time: 1134.9 s\n",
      "Epoch: 4  | Loss training: 0.416 | LR: 0.00006 | Training time: 1180.4 s\n",
      "Epoch: 5  | Loss training: 0.404 | LR: 0.00003 | Training time: 1170.1 s\n",
      "Total training time: 5809.1 s\n"
     ]
    }
   ],
   "source": [
    "# Construct model\n",
    "set_random_seed(seed=seed)\n",
    "lstm_model = Cuda_MDN_LSTM(model_hyper_parameters).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(),\n",
    "                             lr=model_hyper_parameters[\"learning_rate\"])\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                            step_size=model_hyper_parameters[\"adapt_learning_rate_epoch\"],\n",
    "                                            gamma=model_hyper_parameters[\"adapt_gamma_learning_rate\"])\n",
    "\n",
    "# Set forget gate to ensure long-term dependency learning\n",
    "lstm_model.lstm.bias_hh_l0.data[model_hyper_parameters['hidden_size']:2 * model_hyper_parameters['hidden_size']] = model_hyper_parameters[\"set_forget_gate\"]\n",
    "\n",
    "training_time = time.time()\n",
    "\n",
    "for epoch in range(1, model_hyper_parameters[\"no_of_epochs\"] + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = []  # List to store all training losses for the current epoch\n",
    "\n",
    "    # Training\n",
    "    lstm_model.train()\n",
    "    for x_lstm, y in train_loader:\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        # Forward pass\n",
    "        pi, mu, sigma = lstm_model(x_lstm.to(device).float())\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = nll_loss(pi, mu, sigma, y.to(device).float())\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add current batch loss to total_loss\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "        # Clear memory\n",
    "        del y, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_loss_training = sum(total_loss) / len(train_loader)\n",
    "\n",
    "    # Save model, optimizer, and scheduler states after each epoch\n",
    "    path_saved_model = f'{path_save_folder}/epoch_{epoch}'\n",
    "    torch.save(lstm_model.state_dict(), path_saved_model)\n",
    "    torch.save(optimizer.state_dict(), f'{path_save_folder}/optimizer_epoch_{epoch}.pt')\n",
    "    torch.save(scheduler.state_dict(), f'{path_save_folder}/scheduler_epoch_{epoch}.pt')\n",
    "\n",
    "    # Print epoch report\n",
    "    epoch_training_time = time.time() - epoch_start_time\n",
    "    LR = optimizer.param_groups[0]['lr']\n",
    "    report = (f'Epoch: {epoch:<2} | Loss training: {\"%.3f\" % average_loss_training} | '\n",
    "              f'LR: {\"%.5f\" % LR} | Training time: {\"%.1f\" % epoch_training_time} s')\n",
    "    print(report)\n",
    "\n",
    "    # Save epoch report in txt file\n",
    "    write_report(file_path=f'{path_save_folder}/run_progress.txt', text=report)\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# Print total report\n",
    "total_training_time = time.time() - training_time\n",
    "report = f'Total training time: {\"%.1f\" % total_training_time} s'\n",
    "print(report)\n",
    "write_report(file_path=f'{path_save_folder}/run_progress.txt', text=report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 7. Test GM-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In case I already trained an LSTM I can re-construct the model\n",
    "#path_save_folder = 'attert_model/saved_epochs/temporal_segmentation/run_12/'\n",
    "#lstm_model = Cuda_MDN_LSTM(model_hyper_parameters).to(device)\n",
    "#lstm_model.load_state_dict(torch.load(path_save_folder + '/epoch_5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches in testing:  687\n",
      "x_lstm: torch.Size([2033, 128, 16]) | y: torch.Size([2033, 1])\n"
     ]
    }
   ],
   "source": [
    "# Dataset testing\n",
    "test_dataset = BaseDataset(dynamic_input=dynamic_input,\n",
    "                           static_input=static_input,\n",
    "                           target=target,\n",
    "                           sequence_length=model_hyper_parameters['seq_length'],\n",
    "                           set_type='test',\n",
    "                           path_entities=path_entities,\n",
    "                           path_data=path_data,\n",
    "                           check_NaN=False)\n",
    "\n",
    "test_dataset.scaler = training_dataset.scaler # read the global statisctics calculated in the training period\n",
    "test_dataset.standardize_data(standardize_output=False)\n",
    "\n",
    "# DataLoader for testing data.\n",
    "test_batches=[[index for index, _ in group] for _ , group in groupby(enumerate(test_dataset.valid_entities), \n",
    "                                                                     lambda x: x[1][0])]\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_sampler=test_batches)\n",
    "\n",
    "# see if the batches are loaded correctly\n",
    "print('Batches in testing: ', len(test_loader))\n",
    "x_lstm, y= next(iter(test_loader))\n",
    "print(f'x_lstm: {x_lstm.shape} | y: {y.shape}')\n",
    "\n",
    "# create some lists with the valid basins and the valid entities per basin that will help later to organize the data\n",
    "valid_location_testing = [next(group)[0] for key, group in groupby(test_dataset.valid_entities, key=lambda x: x[0])]\n",
    "valid_entity_per_location_testing = [[id for _, id in group] for key, group in groupby(test_dataset.valid_entities, \n",
    "                                                                                    key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No common items found.\n"
     ]
    }
   ],
   "source": [
    "# Double check if there are common items (i.e. location segments in training and testing sets)\n",
    "def find_common_items(list1, list2):\n",
    "    # Find common items using set intersection\n",
    "    common_items = set(list1) & set(list2)\n",
    "    if common_items:\n",
    "        print(f\"Common items found: {common_items}\")\n",
    "    else:\n",
    "        print(\"No common items found.\")\n",
    "\n",
    "\n",
    "# Check for common items\n",
    "find_common_items(valid_location_testing, valid_locations_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pi_values: (1396671, 3)\n",
      "Shape of mu_values: (1396671, 3)\n",
      "Shape of sigma_values: (1396671, 3)\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store pi, mu, sigma for all locations\n",
    "pi_values_list = []\n",
    "mu_values_list = []\n",
    "sigma_values_list = []\n",
    "\n",
    "# Initialize test_results list\n",
    "test_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (x_lstm, y) in enumerate(test_loader):\n",
    "        # Run LSTM prediction\n",
    "        pi, mu, sigma = lstm_model(x_lstm.to(device).float())\n",
    "\n",
    "        # Scale mu and sigma back to the original scale\n",
    "        mu = mu * test_dataset.scaler['y_std'].to(device) + test_dataset.scaler['y_mean'].to(device)\n",
    "        sigma = sigma * test_dataset.scaler['y_std'].to(device)\n",
    "\n",
    "        # Extract corresponding dataframe for the current location/entity\n",
    "        df_ts = test_dataset.df_ts[valid_location_testing[i]].iloc[valid_entity_per_location_testing[i]]\n",
    "\n",
    "        # Create a copy of the slice to avoid SettingWithCopyWarning\n",
    "        df_ts_copy = df_ts.copy().reset_index()\n",
    "\n",
    "        # Assign pi, mu, sigma values to the copied slice - adjust the number of pi, mu, and sigmas according to number of mixtures used in the GM-LSTM model\n",
    "        df_ts_copy['pi_1'] = pi[:, 0].cpu().detach().numpy()\n",
    "        df_ts_copy['pi_2'] = pi[:, 1].cpu().detach().numpy()\n",
    "        df_ts_copy['pi_3'] = pi[:, 2].cpu().detach().numpy()\n",
    "\n",
    "        df_ts_copy['mu_1'] = mu[:, 0].cpu().detach().numpy()\n",
    "        df_ts_copy['mu_2'] = mu[:, 1].cpu().detach().numpy()\n",
    "        df_ts_copy['mu_3'] = mu[:, 2].cpu().detach().numpy()\n",
    "\n",
    "        df_ts_copy['sigma_1'] = sigma[:, 0].cpu().detach().numpy()\n",
    "        df_ts_copy['sigma_2'] = sigma[:, 1].cpu().detach().numpy()\n",
    "        df_ts_copy['sigma_3'] = sigma[:, 2].cpu().detach().numpy()\n",
    "\n",
    "        # Store df_ts_copy in test_results list\n",
    "        test_results.append(df_ts_copy)\n",
    "\n",
    "        # Store pi, mu, sigma values in lists for later concatenation\n",
    "        pi_values_list.append(pi.cpu().detach().numpy())\n",
    "        mu_values_list.append(mu.cpu().detach().numpy())\n",
    "        sigma_values_list.append(sigma.cpu().detach().numpy())\n",
    "\n",
    "# Concatenate pi, mu, sigma values across all timesteps\n",
    "pi_values = np.concatenate(pi_values_list, axis=0)\n",
    "mu_values = np.concatenate(mu_values_list, axis=0)\n",
    "sigma_values = np.concatenate(sigma_values_list, axis=0)\n",
    "\n",
    "# Check shapes for debugging\n",
    "print(f'Shape of pi_values: {pi_values.shape}')\n",
    "print(f'Shape of mu_values: {mu_values.shape}')\n",
    "print(f'Shape of sigma_values: {sigma_values.shape}')\n",
    "\n",
    "# Convert test_results to a dictionary for easier access\n",
    "test_results_dict = {valid_location_testing[i]: result for i, result in enumerate(test_results)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all test results in a comnbined dataframe\n",
    "filtered_dfs=[]\n",
    "\n",
    "# Loop through each location and filter the dataframe\n",
    "for location, df_ts in test_results_dict.items():\n",
    "    filtered_df = df_ts[['date', 'VWC', 'P', 'ET', 'T', 'Q', 'ERA5_VWC',\n",
    "                         'pi_1', 'pi_2', 'pi_3',\n",
    "                         'sigma_1', 'sigma_2', 'sigma_3',\n",
    "                         'mu_1', 'mu_2', 'mu_3']].copy() # select any\n",
    "    \n",
    "    # Append filtered dataframe to the list\n",
    "    filtered_dfs.append(filtered_df)\n",
    "\n",
    "# Concatenate all filtered dataframes into one large dataframe\n",
    "total_locations_df = pd.concat(filtered_dfs, ignore_index=True).dropna()\n",
    "\n",
    "# Save the large dataframe to a CSV file\n",
    "total_locations_df.to_csv('', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and save for all different locations (i.e. segments)\n",
    "filtered_results_dict = {}\n",
    "\n",
    "for location, df_ts in test_results_dict.items():\n",
    "    filtered_df = df_ts[['date', 'VWC', 'P', 'ET', 'T', 'Q', 'ERA5_VWC',\n",
    "                         'pi_1', 'pi_2', 'pi_3',\n",
    "                         'sigma_1', 'sigma_2', 'sigma_3',\n",
    "                         'mu_1', 'mu_2', 'mu_3']].copy() # selecty any\n",
    "    \n",
    "    # Store filtered dataframe in the new dictionary\n",
    "    filtered_results_dict[location] = filtered_df\n",
    "\n",
    "# Save filtered_results_dict to a file (e.g., CSV)\n",
    "for location, df in filtered_results_dict.items():\n",
    "    df.to_csv(f'', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "42b7dc197ee81dd2f6541889b0e14556b882d218c1e7c97db94bc0f7b191f034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
